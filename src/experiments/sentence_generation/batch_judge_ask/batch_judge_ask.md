# Batch Judge Ask

## Overview

`batch_judge_ask` is an experiment designed to generate pharmacogenomic association sentences for a set of genetic variants from a single scientific article (identified by its PMCID). Unlike the `llm_judge_ask` experiment which processes each variant individually, this script batches all variants for a given PMCID into a single call to a large language model (LLM).

The primary goals of this experiment are:
- To evaluate the ability of LLMs to extract multiple structured data points from a single text in one pass.
- To improve the efficiency of sentence generation by reducing the number of LLM API calls.
- To assess whether batching improves the consistency of generated sentences for related variants within the same paper.

## How It Works

1.  **Input**: The script takes a list of PMCIDs and their associated variants from the `variant_bench.jsonl` file.
2.  **Article Text**: For each PMCID, the script fetches the full text of the article.
3.  **Batch Prompting**: It constructs a single prompt containing the article text and the entire list of variants for that PMCID.
4.  **LLM Generation**: The script sends this batch prompt to the specified LLM (e.g., GPT-4, Claude).
5.  **Parsing**: The LLM's response, which should contain sentences for all variants, is parsed into a structured JSON file.
6.  **Evaluation**: The generated sentences are then optionally evaluated against a ground-truth benchmark for accuracy and completeness.

## Sample Commands

Here are some examples of how to run the experiment:

**1. Run with default settings:**
This will process one PMCID using the `gpt-5` model and the `v3` prompt.
```bash
python batch_judge_ask.py
```

**2. Specify model and prompt:**
Run with a different model and prompt, and process a single PMCID.
```bash
python batch_judge_ask.py --model gpt-4o-mini --prompt v4 --num-pmcids 1
```

**3. Run without evaluation:**
Generate sentences but skip the automatic evaluation step.
```bash
python batch_judge_ask.py --no-eval
```

**4. Specify a different judge model:**
Use a different model for the evaluation step.
```bash
python batch_judge_ask.py --model gpt-4o-mini --prompt v4 --judge-model claude-3-haiku-20240307
```

## Prompts

The experiment uses prompts defined in `prompts.yaml`:

-   **`v3`**: A prompt that asks the model to generate a single sentence for each variant, describing the association.
-   **`v4`**: An extended prompt that asks for both an association sentence and a brief explanation of the evidence for each variant.

## Output and Evaluation

The script generates two types of files:

1.  **Generated Sentences (`outputs/`)**: A JSON file containing the sentences (and explanations, if using `v4`) generated by the LLM for each variant. The filename includes the model, prompt, and timestamp (e.g., `gpt-4o-mini_v4_20260120_004717.json`).

    *Example Output Structure (`v4`):*
    ```json
    {
      "PMC5508045": {
        "rs9923231": [
          {
            "sentence": "Genotypes AA + AG of rs9923231 are associated with decreased dose of warfarin...",
            "explanation": "The study found that the AA and AG genotypes required significantly lower warfarin doses..."
          }
        ]
      }
    }
    ```

2.  **Evaluation Results (`results/`)**: A JSON file containing the scores of the generated sentences compared to the ground truth. The filename includes the judge model and timestamp (e.g., `sentence_scores_llm_claude-3-haiku-20240307_20260120_004717.json`).

    *Example Results Structure:*
    ```json
    {
      "overall_avg_score": 0.8,
      "num_pmcids": 1,
      "per_pmcid": [
        {
          "pmcid": "PMC5508045",
          "avg_score": 0.8,
          "per_variant": [
            {
              "variant": "rs9923231",
              "score": 0.8,
              ...
            }
          ]
        }
      ]
    }
    ```

## Results

To generate results, run the experiment with your desired models and prompts. The output and evaluation files will be saved in the `outputs/` and `results/` directories, respectively. You can then analyze these files to compare the performance of different models and prompts.