# Evaluation Pipeline
This should run and create a comprehensive evaluation of the pipeline outputs. Draw on what we've done for the benchmark_v2 and the experiment evaluation scripts.
Judging models and prompts should be configurable via a config file. We should have reports for whether the variants were correctly identified and whether the sentences were correctly generated similar to what we've done in the benchmark_v2 and experiment evaluation scripts.
